{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\priyanka kaushal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cryptography-44.0.1-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\priyanka kaushal\\appdata\\roaming\\python\\python312\\site-packages (from pytesseract) (24.2)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 670.4 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 684.4 kB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 0.8/5.6 MB 684.4 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 1.0/5.6 MB 762.8 kB/s eta 0:00:06\n",
      "   --------- ------------------------------ 1.3/5.6 MB 808.5 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 864.6 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 1.8/5.6 MB 923.6 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 2.1/5.6 MB 970.4 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 986.9 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 2.6/5.6 MB 986.7 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 2.9/5.6 MB 1.0 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.1/5.6 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 3.4/5.6 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 3.7/5.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 3.9/5.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.7/5.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.0/5.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.2/5.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.6 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.3/2.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.6/2.6 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.6 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/3.0 MB 1.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/3.0 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.0/3.0 MB 968.5 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.0/3.0 MB 968.5 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 987.4 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.6/3.0 MB 953.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.8/3.0 MB 996.7 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.4/3.0 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading cryptography-44.0.1-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.8/3.2 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.0/3.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.6/3.2 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.1/3.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.9/3.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.1/3.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, Pillow, pytesseract, pdf2image, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-11.1.0 cffi-1.17.1 cryptography-44.0.1 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.5 pycparser-2.22 pypdfium2-4.30.1 pytesseract-0.3.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pypdfium2.exe is installed in 'c:\\Users\\Priyanka Kaushal\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pytesseract.exe is installed in 'c:\\Users\\Priyanka Kaushal\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pdfplumber.exe is installed in 'c:\\Users\\Priyanka Kaushal\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfplumber pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try direct text extraction\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Direct text extraction failed: {e}\")\n",
    "\n",
    "    # Fallback to OCR for image-based PDFs\n",
    "    print(\"Falling back to OCR for image-based PDF.\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed: {e}\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Text from PDF:\n",
      "Priyanka Kaushal\n",
      "(089)9599455 | priyanka.kaushal11@gmail.com | LinkedIn | Dublin, Ireland\n",
      "SUMMARY\n",
      "Senior Data Engineer with 9 years of experience in designing and optimizing data pipelines, ETL processes, and data\n",
      "solutions. Currently pursuing a Master’s in Data and Computational Science at University College Dublin.\n",
      "• Expertise in implementation of ETL applications, Data warehousing/ data modelling principles, architecture\n",
      "leveraging SQL, Python, SSIS, Azure Data Factory & Airflow.\n",
      "• Skilled in data governance, quality assurance, and building scalable solutions for large-scale environments.\n",
      "• Currently expanding knowledge on big data technologies and public clouds like Azure.\n",
      "• Strong background in Agile methodologies (SCM certified), team leadership, and delivering data-driven insights\n",
      "to align with business goals.\n",
      "SKILLS\n",
      "• ETL Tools: Microsoft SSIS, Airflow, Azure Data Factory\n",
      "• Databases: SQL Server, MySQL, PostgreSQL, Oracle, Azure Postgres\n",
      "• Programming Languages: Python, R, SQL\n",
      "• Visualization Tools: Tableau, Power BI, MS-Excel\n",
      "• Version Control: Git, Bitbucket\n",
      "WORK EXPERIENCE\n",
      "Dell Technologies Nov '18 — Aug’24\n",
      "Senior Data Engineer\n",
      "Projects: Market Intelligence Modernization, Procurement Analytics\n",
      "• Developed automated data cleaning and standardization processes, reducing data errors by 40% and improving\n",
      "data quality for downstream analytics using SQL ,posgresSQL ,SSIS, and Python.\n",
      "• Designed and orchestrated data pipelines using Azure Data Factory/SSIS and Airflow, processing terabytes of\n",
      "data daily, ensuring efficient ingestion, cleaning, and transformation of data for business-critical applications.\n",
      "• Migrated 15+ data pipelines from SQL Server to PostgreSQL, reducing operational costs by 20%.\n",
      "• Built and maintained Postgres databases, implementing OLTP optimization tactics to handle 10,000+ concurrent\n",
      "updates per second, ensuring high availability and scalability.\n",
      "• Collaborated with cross-functional teams to define KPI metrics and delivered data-driven insights, resulting in\n",
      "a 15% increase in revenue.\n",
      "Achievements:\n",
      "• Awarded for developing an Inventory Optimization tool, reducing inventory shortages by 11% YoY.\n",
      "Mindtree Oct '15 — Nov '18\n",
      "Data Engineer\n",
      "Projects: Inventory Stock Analysis, College Application Trend Analysis\n",
      "• Developed ETL solutions using SSIS to extract and integrate data from 10+ sources, improving data integration\n",
      "efficiency by 20%.\n",
      "• Optimized SQL queries, reducing error rates by 50% and saving ~10 hours per week in query execution time.\n",
      "• Collaborated with clients to gather requirements and resolve data issues, ensuring 100% alignment with business\n",
      "objectives.\n",
      "• Automated data workflows, reducing manual effort by 30% and improving data processing speed by 25%.\n",
      "Achievements:\n",
      "Spot on Award for optimizing test queries, reducing processing time by 10 hours per week.\n",
      "EDUCATION\n",
      "MSc in Data & Computational Science, University College Dublin Sep '24 — Present\n",
      "Key Modules: Data Science with Python, R Programming, Modern Regression Analysis, Applied Matrix Algebra &\n",
      "Statistics, Optimization in ML, Artificial Intelligence, Machine Learning, Bayesian and Multivariate Analysis, SAS\n",
      "• GPA – 3.83\n",
      "Bachelor of Technology in Computer Science, B.P. Poddar Institute of Management & Technology Aug '11 — Jun '15\n",
      "• DGPA – 8.7/10\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Resume.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nExtracted Text from PDF:\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google.generativeai \n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx')\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is **New Delhi**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resume(resume_text, job_description=None):\n",
    "    if not resume_text:\n",
    "        return {\"error\": \"Resume text is required for analysis.\"}\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    base_prompt = f\"\"\"\n",
    "    You are an experienced HR with Technical Experience in the field of any one job role from Data Science, Data Analyst, DevOPS, Machine Learning Engineer, Prompt Engineer, AI Engineer, Full Stack Web Development, Big Data Engineering, Marketing Analyst, Human Resource Manager, Software Developer your task is to review the provided resume.\n",
    "    Please share your professional evaluation on whether the candidate's profile aligns with the role.ALso mention Skills he already have and siggest some skills to imorve his resume , alos suggest some course he might take to improve the skills.Highlight the strengths and weaknesses.\n",
    "\n",
    "    Resume:\n",
    "    {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "    if job_description:\n",
    "        base_prompt += f\"\"\"\n",
    "        Additionally, compare this resume to the following job description:\n",
    "        \n",
    "        Job Description:\n",
    "        {job_description}\n",
    "        \n",
    "        Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements.\n",
    "        \"\"\"\n",
    "\n",
    "    response = model.generate_content(base_prompt)\n",
    "\n",
    "    analysis = response.text.strip()\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Resume Review for Priyanka Kaushal - Senior Data Engineer\n",
      "\n",
      "**Overall Impression:** Priyanka's resume demonstrates a strong background as a Senior Data Engineer with a clear progression in her career and quantifiable achievements.  Her experience aligns well with typical Senior Data Engineer roles, though some improvements can enhance her candidacy.\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Quantifiable Achievements:** Priyanka consistently uses numbers to showcase the impact of her work (e.g., 40% reduction in data errors, 20% reduction in operational costs, 15% increase in revenue). This is excellent and demonstrates a results-oriented approach.\n",
      "* **Diverse Skillset:**  Her skills encompass ETL processes (SSIS, Airflow, ADF), databases (SQL Server, PostgreSQL, MySQL, Oracle), programming languages (Python, R, SQL), visualization tools (Tableau, Power BI), and version control (Git, Bitbucket). This breadth of knowledge is valuable.\n",
      "* **Experience in Large-Scale Environments:**  She mentions working with terabytes of data and handling 10,000+ concurrent updates per second, indicating experience with high-volume data processing.\n",
      "* **Agile Methodology Experience:** SCM certification showcases familiarity with Agile development practices.\n",
      "* **Ongoing Education:** Pursuing a Master's degree demonstrates a commitment to continuous learning and staying up-to-date with the latest technologies.  The relevant coursework further strengthens this point.\n",
      "* **Clear Project Descriptions:** Projects are briefly explained, highlighting key responsibilities and results.\n",
      "\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Lack of Specific Cloud Skills:** While mentioning Azure Data Factory and Azure Postgres, the resume doesn't explicitly detail expertise in other Azure services (e.g., Azure Databricks, Azure Synapse Analytics).  In today's market, deeper cloud expertise is a significant asset.\n",
      "* **Resume Length:**  The resume could be slightly more concise.  Some bullet points could be merged or shortened without losing key information.\n",
      "* **Missing Keywords:** The resume could benefit from incorporating more industry-specific keywords for Applicant Tracking Systems (ATS) to pick up. For example,  terms like \"Data Lake,\" \"Data Modeling,\" \"ETL Optimization,\" \"Dimensional Modeling,\" or specific big data technologies (Spark, Hadoop) could be strategically added.\n",
      "* **Limited Detail on Master's Thesis/Projects:** The resume could mention the topic of her Master's thesis or any significant projects undertaken during her Master's program. This would showcase advanced skills and research capabilities.\n",
      "\n",
      "\n",
      "**Skills Priyanka Already Has:**\n",
      "\n",
      "* **ETL:** SSIS, Airflow, Azure Data Factory\n",
      "* **Databases:** SQL Server, MySQL, PostgreSQL, Oracle, Azure Postgres\n",
      "* **Programming:** Python, R, SQL\n",
      "* **Visualization:** Tableau, Power BI\n",
      "* **Version Control:** Git, Bitbucket\n",
      "* **Cloud (Basic):** Azure Data Factory, Azure Postgres\n",
      "* **Agile:** SCM Certified\n",
      "\n",
      "\n",
      "**Skills to Improve/Add to Resume:**\n",
      "\n",
      "* **Advanced Cloud Skills:** Azure Databricks, Azure Synapse Analytics, AWS (if applicable), GCP (if applicable) – specific services and hands-on experience.\n",
      "* **Big Data Technologies:** Spark, Hadoop, Hive, Kafka –  demonstrate familiarity and experience.\n",
      "* **Data Modeling Techniques:** Star schema, Snowflake schema, dimensional modeling – specify these explicitly.\n",
      "* **Data Warehousing:**  Detail specific experience with data warehouse design and implementation.\n",
      "* **Data Governance and Compliance:**  Mention specific regulations or frameworks (e.g., GDPR, CCPA) if applicable.\n",
      "* **Specific Keywords:**  Incorporate more keywords related to data engineering roles and specific technologies used.\n",
      "\n",
      "\n",
      "**Suggested Courses:**\n",
      "\n",
      "* **Cloud-Specific Certifications:**  Microsoft Azure Data Engineer Associate, AWS Certified Data Analytics - Specialty, Google Cloud Certified - Professional Data Engineer. These certifications would significantly boost her resume.\n",
      "* **Big Data Technologies Courses:** Courses on Spark, Hadoop, and other big data technologies from Coursera, edX, Udacity, or DataCamp.\n",
      "* **Advanced Data Modeling and Warehousing:**  Online courses or workshops focused on data modeling techniques and data warehouse design.\n",
      "* **Data Governance and Compliance:** Courses or certifications related to data governance and relevant compliance regulations.\n",
      "\n",
      "\n",
      "**Alignment with the Role:**\n",
      "\n",
      "Priyanka's profile strongly aligns with a Senior Data Engineer role.  However, enhancing her resume with the suggested improvements—particularly in cloud and big data technologies—will significantly strengthen her candidacy and make her stand out from other applicants.  The master's degree is a great asset but needs to be leveraged more effectively to showcase specific accomplishments during the program.\n"
     ]
    }
   ],
   "source": [
    "print(analyze_resume(resume_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
